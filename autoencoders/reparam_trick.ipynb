{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fc1fcfa-5632-4623-b975-1404bf7ae9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2139ffc61d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6410b6a-43f7-4bf0-a15e-3226527f4476",
   "metadata": {},
   "source": [
    "The point of this notebook is to better understand the reparamterization trick which is really one of the biggest tricks in the VAE. Per the point made [here](https://gregorygundersen.com/blog/2018/04/29/reparameterization/) this notebook demonstrates that torch can \"differentiate through random nodes\" and it even does it fine, the real necessity for the reparam trick is that when your distribution and function depend on the same parameters you get a product rule in the expectation. Unless you can evaluate things analytically like in my toy example, you will always get a nonsensical estimate for the gradient of this expectation using auto grad.\n",
    "\n",
    "First we compute an expectation that torch can handle\n",
    "$$\\nabla_\\theta \\mathbb{E}_{p(z)}[f_\\theta(z)] = \\int_z p(z) \\nabla_\\theta f_\\theta(z) dz = \\mathbb{E}_{p(z)}[\\nabla_\\theta f_\\theta(z)]$$\n",
    "**the key thing here is that this is only true when $p_{(z)}$ does not also depend on $\\theta$**\n",
    "\n",
    "Now define\n",
    "$$p_(z) = N(z|1,0)$$\n",
    "$$f_\\theta(z) = 3\\theta z^2$$\n",
    "\n",
    "We can compute these expectations analytically and see they should be 3, next we compute them two ways $\\nabla_\\theta \\mathbb{E}_{p(z)}[f_\\theta(z)], \\mathbb{E}_{p(z)}[\\nabla_\\theta f_\\theta(z)]$ the first using autograd in pytorch the second by giving the gradient function to monte carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "10e2484d-6934-41a6-aa1f-78c17d055f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: grad E[f] = 3.007117748260498\n",
      "MC: E[grad f] = 2.9939215657133142\n"
     ]
    }
   ],
   "source": [
    "def f(t,z):\n",
    "    return 3*t*z**2\n",
    "\n",
    "def grad_f(z):\n",
    "    return 3*z**2\n",
    "\n",
    "N = 10000\n",
    "xs = torch.randn(N)\n",
    "t = torch.Tensor([3])\n",
    "t.requires_grad = True\n",
    "y = 3*t*xs**2\n",
    "y = torch.mean(y)\n",
    "y.backward()\n",
    "print(f\"torch: grad E[f] = {t.grad.item()}\")\n",
    "\n",
    "o_xs = np.random.randn(N)\n",
    "print(f\"MC: E[grad f] = {np.mean(grad_f(o_xs))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1e190-d3ef-4ab8-8d90-22e77d42ed83",
   "metadata": {},
   "source": [
    "Now we instead let $p(z)$ depend on theta\n",
    "$$p_\\theta(z) = N(z|\\theta,0)$$\n",
    "$$f_\\theta(z) = 3\\theta z^2$$\n",
    "\n",
    "This gives a totally different equation for the gradient of the expectation\n",
    "$$\\nabla_\\theta \\mathbb{E}_{p_\\theta(z)}[f_\\theta(z)] = \\int_z f_\\theta(z)\\nabla_\\theta p_\\theta(z) dz + \\mathbb{E}[\\nabla_\\theta f_\\theta(z)]$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\int_z f_\\theta(z)\\nabla_\\theta p_\\theta(z) dz + \\int_z \\nabla_\\theta f_\\theta(z) p_\\theta(z) dz = -6t^2 + 30$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "25b2e77f-ccc8-4258-89fb-a8055be20dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyatic grad E_t[f] = -24\n",
      "MC E[grad f] = -23.982790434748306\n",
      "Autograd E_t[f] = 30.004558563232422\n"
     ]
    }
   ],
   "source": [
    "N = 1000000\n",
    "\n",
    "theta = 3\n",
    "t = torch.Tensor([theta])\n",
    "t.requires_grad = True\n",
    "xs = torch.normal(mean=t.repeat(1,N).squeeze(),std=torch.ones(N))\n",
    "y = 3*t*xs**2\n",
    "y = torch.mean(y)\n",
    "y.backward()\n",
    "\n",
    "\n",
    "print(f\"Analyatic grad E_t[f] = {-6*theta**2 + 30}\")\n",
    "o_xs = np.random.normal(loc=theta,size=N)\n",
    "print(f\"MC E[grad f] = {-6*theta**2 + np.mean(grad_f(o_xs))}\")\n",
    "print(f\"Autograd E_t[f] = {t.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221df79-4d46-4204-82cd-52850aafbb0d",
   "metadata": {},
   "source": [
    "We can see that the autograd will **never** get close to estimating the expectation, because it simply isnt dealing with the right quantity anymore.\n",
    "This gets at the heart of the real point of the paramterization trick, if $g_\\theta$ is differentiable then we define \n",
    "$$\\epsilon \\sim p(\\epsilon)$$\n",
    "$$z = g_\\theta(\\epsilon,x)$$\n",
    "Since $g$ is deterministic in $x$ we can swap out the distribution in our expectation for $p(\\epsilon)$ which no longer depends on $\\theta$\n",
    "$$\\nabla_\\theta \\mathbb{E}_{p_\\theta(z)}[f(z)] = \\nabla_\\theta \\mathbb{E}_{p_\\epsilon}[f(g_\\theta(\\epsilon,x))] =  \\mathbb{E}_{p_\\epsilon}[\\nabla_\\theta f(g_\\theta(\\epsilon,x))]$$\n",
    "\n",
    "as shown [here](https://nbviewer.org/github/gokererdogan/Notebooks/blob/master/Reparameterization%20Trick.ipynb) this not only makes it so we can use backprop, this estimator also seems to have nicer statistical properties like less variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f20245-8134-4810-90ad-4285be39e39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
